{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation in Multilayer Neural Networks\n",
    "\n",
    "### Goals: \n",
    "- Diving deep: implement a real gradient descent in `Numpy`\n",
    "\n",
    "### Dataset:\n",
    "- Similar as first Lab - Digits: 10 class handwritten digits\n",
    "- [sklearn.datasets.load_digits](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMIAAADSCAYAAAAYNx+qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAADOFJREFUeJzt3XuMHWUdxvHvw3IrUqjSSqAFFoUSwISt1hpC0MrNIoVWYyJVMdtIIBqEDSYg/oGrf2hMDLYaI2IBG8tFyqVeAiLGNkoiSFuKUAqklMWuRXYbW1tQgdKff8xscrrd7b5b5nLO7vNJTnp2Z847v92eZ985M/POq4jAbLw7oO4CzJqBg2CGg2AGOAhmgINgBjgIZsA4DIKk9ZJm113HvkjqlPRo4rrdkpbt53b2+7VjzbgLQkScHhGr6q6j1Ug6TdJqSdvyxx8knVZ3XUUZd0Gw/bYF+AzwHmAy8Gvg7lorKtC4C4KkHknn5c+7JS2XtEzSTklPS5ou6QZJfZI2S7qg4bULJW3I190k6cpBbV8n6RVJWyRdLikknZQvO0TS9yX9XdKrkm6WNCGx5sV5LTskrZF09qBVDpX0y7yutZLOaHjtsZLuk9Qv6SVJV+/P7y0itkdET2SXIgh4Gzhpf9pqRuMuCEO4GPgF8G7gSeBhst/LVODbwE8b1u0D5gJHAAuBH0j6IICkOcC1wHlkb5CPDdrO94DpQEe+fCpwY2KNT+Svew9wJ7Bc0qENy+cByxuWr5B0kKQDgN8AT+XbOxfokvSJoTYi6W+SPrevQiRtB/4H/Aj4TmL9zS8ixtUD6AHOy593A480LLsYeA1oy7+eCAQwaZi2VgDX5M9vA77bsOyk/LUnkf0FfR14f8PyM4GXhmm3E3h0Hz/DNuCMhp/hsYZlBwCvAGcDHwH+Pui1NwC3N7x22X78Dt8FfAW4qO7/z6IeBxYTp5b2asPz/wJbI+Lthq8BDge2S7oQ+CbZX/YDgMOAp/N1jgVWN7S1ueH5lHzdNZIGviegLaVASV8DLs+3EWQ90uShthURuyX1Nqx7bP5XfEAb8OeU7Q4nIl6XdDPQL+nUiOh7J+01AwchkaRDgPuALwK/ioi3JK0ge0ND9ld4WsNLjmt4vpUsVKdHxD9Gud2zgevJdmvW52/0bQ3b3WNb+e7QNLIPt7vIep2TR7PNRAN/CKaS7TK2NH9GSHcwcAjQD+zKe4cLGpbfAyyUdKqkw2jY/4+I3cDPyD5TvBdA0tTh9tUHmUj2hu4HDpR0I1mP0OhDkj4t6UCgC3gDeAz4K7BD0vWSJkhqk/QBSR8e7Q8v6XxJM/I2jgBuIttF2zDatpqRg5AoInYCV5O94bcBnyM7hDiw/CHgh8BKYCPwl3zRG/m/1+fff0zSDuAPwCkJm34YeAh4AXiZ7IPq5kHr/Ar4bF7XZcCnI+KtfBfvYrIP2i+R9UxLgCOH2lB+svHzw9QxCbgL+DfwItlnnzkR8b+En6HpKf/wYwWTdCrwDHBIROyqux7bN/cIBZL0KUkHS3o32eHS3zgErcFBKNaVZPvyL5KdcPpyveVYKu8ameEewQxwEMyAkk6oTZ48Odrb28toujCbNw8+AvnO9PUVf05pwoSka/KSHX300YW2B3DUUUcV3maRenp62Lp1q0Zar5QgtLe3s3r16pFXrFFXV1eh7S1evLjQ9gCmT59eaHtF/8wAnZ2dhbdZpJkzZyat510jMxwEM8BBMAMcBDMgMQiS5kh6XtJGSV8vuyizqo0YBEltwI+BC4HTgAVj6e4FZpDWI8wCNkbEpoh4k+zOBfPKLcusWilBmMqe17/35t8zGzNSgjDUWbm9rtSTdEV+A6jV/f3977wyswqlBKGXPcffDoyH3UNE3BIRMyNi5pQpU4qqz6wSKUF4AjhZ0omSDgYupWGIotlYMOK1RhGxS9JVZGNn24DbImJ96ZWZVSjporuIeBB4sORazGrjM8tmOAhmgINgBjgIZsA4vvdpR0dHoe2tWLGi0PYA5s+fX2h7CxcuLLQ9aP4RaqncI5jhIJgBDoIZ4CCYAQ6CGeAgmAEOghmQNmb5tnzO4WeqKMisDik9ws+BOSXXYVarEYMQEX8C/lVBLWa1KewzgscsWysrLAges2ytzEeNzHAQzIC0w6d3kU2efYqkXklfKr8ss2ql3MViQRWFmNXJu0ZmOAhmgINgBjgIZsA4Hrxf9KDz7u7uQtsDOPLIIwttb+nSpYW2N5a4RzDDQTADHAQzwEEwAxwEM8BBMAPSLro7TtJKSRskrZd0TRWFmVUp5TzCLuBrEbFW0kRgjaRHIuLZkmszq0zKmOVXImJt/nwnsAHPs2xjzKg+I0hqB2YAj5dRjFldkoMg6XDgPqArInYMsdyD961lJQVB0kFkIbgjIu4fah0P3rdWlnLUSMCtwIaIuKn8ksyql9IjnAVcBpwjaV3++GTJdZlVKmXM8qOAKqjFrDY+s2yGg2AGOAhmgINgBozjMctFmzFjRuFtTpo0qdD2TjjhhELbG0vcI5jhIJgBDoIZ4CCYAQ6CGeAgmAEOghmQdhn2oZL+KumpfPD+t6oozKxKKSfU3gDOiYjX8gE6j0p6KCIeK7k2s8qkXIYdwGv5lwfljyizKLOqpQ7VbJO0DugDHomIvQbve8yytbKkIETE2xHRAUwDZkn6wBDreMyytaxRHTWKiO3AKmBOKdWY1STlqNEUSZPy5xOA84Dnyi7MrEopR42OAZZKaiMLzj0R8dtyyzKrVspRo7+R3d3ObMzymWUzHAQzwEEwAxwEM8CD9wszb968wttcuXJloe3Nnj270PYA1q1bV2h77e3thbaXyj2CGQ6CGeAgmAEOghngIJgBDoIZMLrJBNskPSnJF9zZmDOaHuEasjmWzcac1KGa04CLgCXllmNWj9QeYRFwHbB7uBU8ZtlaWcoItblAX0Ss2dd6HrNsrSx1etlLJPUAd5NNM7us1KrMKjZiECLihoiYFhHtwKXAHyPiC6VXZlYhn0cwY5SXYUfEKrLbuZiNKe4RzHAQzAAHwQxwEMwAj1luaosWLSq0vZ6enkLbA+js7Cy0vVWrVhXaXir3CGY4CGaAg2AGOAhmgINgBjgIZkDi4dP8EuydwNvAroiYWWZRZlUbzXmEj0fE1tIqMauRd43MSA9CAL+XtEbSFWUWZFaH1F2jsyJii6T3Ao9Iei4i/tS4Qh6QKwCOP/74gss0K1fqhONb8n/7gAeAWUOs48H71rJS7mLxLkkTB54DFwDPlF2YWZVSdo2OBh6QNLD+nRHxu1KrMqtYyjzLm4AzKqjFrDY+fGqGg2AGOAhmgINgBjgIZsA4Hrxf9CDxMgadFz2Zdxk1dnR0FN5mHdwjmOEgmAEOghngIJgBDoIZ4CCYAenTy06SdK+k5yRtkHRm2YWZVSn1PMJi4HcR8RlJBwOHlViTWeVGDIKkI4CPAp0AEfEm8Ga5ZZlVK2XX6H1AP3C7pCclLclHqu3BE45bK0sJwoHAB4GfRMQM4HXg64NX8phla2UpQegFeiPi8fzre8mCYTZmpEw4/k9gs6RT8m+dCzxbalVmFUs9avRV4I78iNEmYGF5JZlVLykIEbEO8I1/bczymWUzHAQzwEEwAxwEM2Acj1kuejLvoscXA7S3txfaXldXV6HtAXR3dxfeZh3cI5jhIJgBDoIZ4CCYAQ6CGeAgmAFpU0edImldw2OHpOKPw5nVKGXGnOeBDgBJbcA/yCYUNBszRrtrdC7wYkS8XEYxZnUZbRAuBe4qoxCzOiUHIR+UcwmwfJjlHrxvLWs0PcKFwNqIeHWohR68b61sNEFYgHeLbIxKveXjYcD5wP3llmNWj9Qxy/8Bjiq5FrPa+MyyGQ6CGeAgmAEOghngIJgBoIgovlGpH0i5HmkysLXwAorV7DU2e31Qb40nRMSIZ3hLCUIqSasjoqlvJdnsNTZ7fdAaNXrXyAwHwQyoPwi31Lz9FM1eY7PXBy1QY62fEcyaRd09gllTqCUIkuZIel7SRkl7TUxYN0nHSVqZT66+XtI1ddc0HElt+Wynv627lqG0ymT1le8a5TcAeIHssu5e4AlgQUQ0zbxsko4BjomItZImAmuA+c1U4wBJ15LNZnRERMytu57BJC0F/hwRSwYmq4+I7XXXNVgdPcIsYGNEbMonL78bmFdDHcOKiFciYm3+fCewAZhab1V7kzQNuAhYUnctQ2mYrP5WyCarb8YQQD1BmApsbvi6lyZ8kw2Q1A7MAB7f95q1WARcB+yuu5BhJE1W3wzqCIKG+F5THrqSdDhwH9AVETvqrqeRpLlAX0SsqbuWfUiarL4Z1BGEXuC4hq+nAVtqqGOfJB1EFoI7IqIZh6ieBVwiqYds9/IcScvqLWkvLTNZfR1BeAI4WdKJ+YenS4Ff11DHsCSJbL92Q0TcVHc9Q4mIGyJiWkS0k/0O/xgRX6i5rD200mT1lU8dFRG7JF0FPAy0AbdFxPqq6xjBWcBlwNOSBuaE+kZEPFhjTa2qJSar95llM3xm2QxwEMwAB8EMcBDMAAfBDHAQzAAHwQxwEMwA+D+6JGdf5pgJaAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_index = 45\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(digits.images[sample_index], cmap=plt.cm.gray_r,\n",
    "           interpolation='nearest')\n",
    "plt.title(\"image label: %d\" % digits.target[sample_index]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "- Normalization\n",
    "- Train / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = np.asarray(digits.data, dtype='float32')\n",
    "target = np.asarray(digits.target, dtype='int32')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, target, test_size=0.15, random_state=37)\n",
    "\n",
    "# mean = 0 ; standard deviation = 1.0\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# print(scaler.mean_)\n",
    "# print(scaler.scale_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1527, 64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(270, 64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1527,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int32')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy Implementation\n",
    "\n",
    "## a) Logistic Regression\n",
    "\n",
    "In this section we will implement a logistic regression model trainable with SGD using numpy. Here are the objectives:\n",
    "\n",
    "- Implement a simple forward model with no hidden layer (equivalent to a logistic regression):\n",
    "note: shape, transpose of W with regards to course\n",
    "$y = softmax(\\mathbf{W} \\dot x + b)$\n",
    "\n",
    "- Build a predict function which returns the most probable class given an input $x$\n",
    "\n",
    "- Build an accuracy function for a batch of inputs $X$ and the corresponding expected outputs $y_{true}$\n",
    "\n",
    "- Build a grad function which computes $\\frac{d}{dW} -\\log(softmax(W \\dot x + b))$ for an $x$ and its corresponding expected output $y_{true}$ ; check that the gradients are well defined\n",
    "\n",
    "- Build a train function which uses the grad function output to update $\\mathbf{W}$ and $b$\n",
    "\n",
    "\n",
    "### One-hot encoding for class label data\n",
    "\n",
    "First let's define a helper function to compute the one hot encoding of an integer array for a fixed number of classes (similar to keras' `to_categorical`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(n_classes, y):\n",
    "    return np.eye(n_classes)[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot(n_classes=10, y=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot(n_classes=10, y=[0, 4, 9, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The softmax function\n",
    "\n",
    "Now let's implement the softmax vector function:\n",
    "\n",
    "$$\n",
    "softmax(\\mathbf{x}) = \\frac{1}{\\sum_{i=1}^{n}{e^{x_i}}}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "  e^{x_1}\\\\\\\\\n",
    "  e^{x_2}\\\\\\\\\n",
    "  \\vdots\\\\\\\\\n",
    "  e^{x_n}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    # TODO:\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that this works one vector at a time (and check that the components sum to one):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(softmax([10, 2, -3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that a naive implementation of softmax might not be able process a batch of activations in a single call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[10, 2, -3],\n",
    "              [-1, 5, -20]])\n",
    "print(softmax(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a way to implement softmax that works both for an individual vector of activations and for a batch of activation vectors at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    exp = np.exp(X)\n",
    "    return exp / np.sum(exp, axis=-1, keepdims=True)\n",
    "\n",
    "\n",
    "print(\"softmax of a single vector:\")\n",
    "print(softmax([10, 2, -3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probabilities should sum to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(softmax([10, 2, -3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sotfmax of 2 vectors:\")\n",
    "X = np.array([[10, 2, -3],\n",
    "              [-1, 5, -20]])\n",
    "print(softmax(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sum of probabilities for each input vector of logits should some to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(softmax(X), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function that given the true one-hot encoded class `Y_true` and and some predicted probabilities `Y_pred` returns the negative log likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(Y_true, Y_pred):\n",
    "    Y_true = np.asarray(Y_true)\n",
    "    Y_pred = np.asarray(Y_pred)\n",
    "    \n",
    "    # TODO\n",
    "    return 0.\n",
    "\n",
    "\n",
    "# Make sure that it works for a simple sample at a time\n",
    "print(nll([1, 0, 0], [.99, 0.01, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the nll of a very confident yet bad prediction is a much higher positive number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nll([1, 0, 0], [0.01, 0.01, .98]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that your implementation can compute the average negative log likelihood of a group of predictions: `Y_pred` and `Y_true` can therefore be past as 2D arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(Y_true, Y_pred):\n",
    "    Y_true = np.atleast_2d(Y_true)\n",
    "    Y_pred = np.atleast_2d(Y_pred)\n",
    "\n",
    "    # TODO\n",
    "    return 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the average NLL of the following 3 almost perfect\n",
    "# predictions is close to 0\n",
    "Y_true = np.array([[0, 1, 0],\n",
    "                   [1, 0, 0],\n",
    "                   [0, 0, 1]])\n",
    "\n",
    "Y_pred = np.array([[0,   1,    0],\n",
    "                   [.99, 0.01, 0],\n",
    "                   [0,   0,    1]])\n",
    "\n",
    "print(nll(Y_true, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/numpy_nll.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now study the following linear model trainable by SGD, **one sample at a time**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.W = np.random.uniform(size=(input_size, output_size),\n",
    "                                   high=0.1, low=-0.1)\n",
    "        self.b = np.random.uniform(size=output_size,\n",
    "                                   high=0.1, low=-0.1)\n",
    "        self.output_size = output_size\n",
    "        \n",
    "    def forward(self, X):\n",
    "        Z = np.dot(X, self.W) + self.b\n",
    "        return softmax(Z)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if len(X.shape) == 1:\n",
    "            return np.argmax(self.forward(X))\n",
    "        else:\n",
    "            return np.argmax(self.forward(X), axis=1)\n",
    "    \n",
    "    def grad_loss(self, x, y_true):\n",
    "        y_pred = self.forward(x)\n",
    "        dnll_output =  y_pred - one_hot(self.output_size, y_true)\n",
    "        grad_W = np.outer(x, dnll_output)\n",
    "        grad_b = dnll_output\n",
    "        grads = {\"W\": grad_W, \"b\": grad_b}\n",
    "        return grads\n",
    "    \n",
    "    def train(self, x, y, learning_rate):\n",
    "        # Traditional SGD update without momentum\n",
    "        grads = self.grad_loss(x, y)\n",
    "        self.W = self.W - learning_rate * grads[\"W\"]\n",
    "        self.b = self.b - learning_rate * grads[\"b\"]      \n",
    "        \n",
    "    def loss(self, x, y):\n",
    "        return nll(one_hot(self.output_size, y), self.forward(x))\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        y_preds = np.argmax(self.forward(X), axis=1)\n",
    "        return np.mean(y_preds == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a model and test its forward inference\n",
    "n_features = X_train.shape[1]\n",
    "n_classes = len(np.unique(y_train))\n",
    "lr = LogisticRegression(n_features, n_classes)\n",
    "\n",
    "print(\"Evaluation of the untrained model:\")\n",
    "train_loss = lr.loss(X_train, y_train)\n",
    "train_acc = lr.accuracy(X_train, y_train)\n",
    "test_acc = lr.accuracy(X_test, y_test)\n",
    "\n",
    "print(\"train loss: %0.4f, train acc: %0.3f, test acc: %0.3f\"\n",
    "      % (train_loss, train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the randomly initialized model on the first example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction(model, sample_idx=0, classes=range(10)):\n",
    "    fig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
    "\n",
    "    ax0.imshow(scaler.inverse_transform(X_test[sample_idx]).reshape(8, 8), cmap=plt.cm.gray_r,\n",
    "               interpolation='nearest')\n",
    "    ax0.set_title(\"True image label: %d\" % y_test[sample_idx]);\n",
    "\n",
    "\n",
    "    ax1.bar(classes, one_hot(len(classes), y_test[sample_idx]), label='true')\n",
    "    ax1.bar(classes, model.forward(X_test[sample_idx]), label='prediction', color=\"red\")\n",
    "    ax1.set_xticks(classes)\n",
    "    prediction = model.predict(X_test[sample_idx])\n",
    "    ax1.set_title('Output probabilities (prediction: %d)'\n",
    "                  % prediction)\n",
    "    ax1.set_xlabel('Digit class')\n",
    "    ax1.legend()\n",
    "    \n",
    "plot_prediction(lr, sample_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training for one epoch\n",
    "learning_rate = 0.01\n",
    "\n",
    "for i, (x, y) in enumerate(zip(X_train, y_train)):\n",
    "    lr.train(x, y, learning_rate)\n",
    "    if i % 100 == 0:\n",
    "        train_loss = lr.loss(X_train, y_train)\n",
    "        train_acc = lr.accuracy(X_train, y_train)\n",
    "        test_acc = lr.accuracy(X_test, y_test)\n",
    "        print(\"Update #%d, train loss: %0.4f, train acc: %0.3f, test acc: %0.3f\"\n",
    "              % (i, train_loss, train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the trained model on the first example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction(lr, sample_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Feedforward Multilayer\n",
    "\n",
    "The objective of this section is to implement the backpropagation algorithm (SGD with the chain rule) on a single layer neural network using the sigmoid activation function.\n",
    "\n",
    "- Implement the `sigmoid` and its element-wise derivative `dsigmoid` functions:\n",
    "\n",
    "$$\n",
    "sigmoid(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "dsigmoid(x) = sigmoid(x) \\cdot (1 - sigmoid(x))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    # TODO\n",
    "    return X\n",
    "\n",
    "\n",
    "def dsigmoid(X):\n",
    "    # TODO\n",
    "    return X\n",
    "\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "plt.plot(x, sigmoid(x), label='sigmoid')\n",
    "plt.plot(x, dsigmoid(x), label='dsigmoid')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/sigmoid.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implement `forward` and `forward_keep_all` functions for a model with a hidden layer with a sigmoid activation function:\n",
    "  - $\\mathbf{h} = sigmoid(\\mathbf{W}^h \\mathbf{x} + \\mathbf{b^h})$\n",
    "  - $\\mathbf{y} = softmax(\\mathbf{W}^o \\mathbf{h} + \\mathbf{b^o})$\n",
    "\n",
    "- Notes: \n",
    "  - try to keep the code as similar as possible as the previous one;\n",
    "  - `forward_keep_activations` is similar to forward, but also returns hidden activations and pre activations;\n",
    "\n",
    "- Update the grad function to compute all gradients; check that the gradients are well defined;\n",
    "\n",
    "- Implement the `train` and `loss` functions.\n",
    "\n",
    "**Bonus**: reimplementing all from scratch only using the lecture slides but without looking at the solution of the `LogisticRegression` is an excellent exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 1e-8\n",
    "\n",
    "\n",
    "class NeuralNet():\n",
    "    \"\"\"MLP with 1 hidden layer with a sigmoid activation\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # TODO\n",
    "        self.W_h = None\n",
    "        self.b_h = None\n",
    "        self.W_o = None\n",
    "        self.b_o = None\n",
    "        self.output_size = output_size\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # TODO\n",
    "        if len(X.shape) == 1:\n",
    "            return np.random.uniform(size=self.output_size,\n",
    "                                     high=1.0-EPSILON, low=EPSILON)\n",
    "        else:\n",
    "            return np.random.uniform(size=(X.shape[0], self.output_size),\n",
    "                                     high=1.0-EPSILON, low=EPSILON)\n",
    "    \n",
    "    def forward_keep_activations(self, X):\n",
    "        # TODO\n",
    "        z_h = 0.\n",
    "        h = 0.\n",
    "        y = np.random.uniform(size=self.output_size,\n",
    "                              high=1.0-EPSILON, low=EPSILON)\n",
    "        return y, h, z_h\n",
    "    \n",
    "    def loss(self, X, y):\n",
    "        # TODO\n",
    "        return 42.\n",
    "\n",
    "    def grad_loss(self, x, y_true):\n",
    "        # TODO\n",
    "        return {\"W_h\": 0., \"b_h\": 0., \"W_o\": 0., \"b_o\": 0.}\n",
    "\n",
    "    def train(self, x, y, learning_rate):\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        if len(X.shape) == 1:\n",
    "            return np.argmax(self.forward(x))\n",
    "        else:\n",
    "            return np.argmax(self.forward(x), axis=1)\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        y_preds = np.argmax(self.forward(X), axis=1)\n",
    "        return np.mean(y_preds == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/neural_net.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 10\n",
    "model = NeuralNet(n_features, n_hidden, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.loss(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.accuracy(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction(model, sample_idx=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, accuracies, accuracies_test = [], [], []\n",
    "losses.append(model.loss(X_train, y_train))\n",
    "accuracies.append(model.accuracy(X_train, y_train))\n",
    "accuracies_test.append(model.accuracy(X_test, y_test))\n",
    "\n",
    "print(\"Random init: train loss: %0.5f, train acc: %0.3f, test acc: %0.3f\"\n",
    "      % (losses[-1], accuracies[-1], accuracies_test[-1]))\n",
    "\n",
    "for epoch in range(15):\n",
    "    for i, (x, y) in enumerate(zip(X_train, y_train)):\n",
    "        model.train(x, y, 0.1)\n",
    "\n",
    "    losses.append(model.loss(X_train, y_train))\n",
    "    accuracies.append(model.accuracy(X_train, y_train))\n",
    "    accuracies_test.append(model.accuracy(X_test, y_test))\n",
    "    print(\"Epoch #%d, train loss: %0.5f, train acc: %0.3f, test acc: %0.3f\"\n",
    "          % (epoch + 1, losses[-1], accuracies[-1], accuracies_test[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.title(\"Training loss\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(accuracies, label='train')\n",
    "plt.plot(accuracies_test, label='test')\n",
    "plt.ylim(0, 1.1)\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction(model, sample_idx=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Exercises\n",
    "\n",
    "### Look at worst prediction errors\n",
    "\n",
    "- Use numpy to find test samples for which the model made the worst predictions,\n",
    "- Use the `plot_prediction` to look at the model predictions on those,\n",
    "- Would you have done any better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/worst_predictions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameters settings\n",
    "\n",
    "- Experiment with different hyper parameters:\n",
    "  - learning rate,\n",
    "  - size of hidden layer,\n",
    "  - initialization scheme: test with 0 initialization vs uniform,\n",
    "  - implement other activation functions,\n",
    "  - implement the support for a second hidden layer.\n",
    "\n",
    "\n",
    "### Mini-batches\n",
    "\n",
    "- The current implementations of `train` and `grad_loss` function currently only accept a single sample at a time:\n",
    "    - implement the support for training with a mini-batch of 32 samples at a time instead of one,\n",
    "    - experiment with different sizes of batches,\n",
    "    - monitor the norm of the average gradients on the full training set at the end of each epoch.\n",
    "\n",
    "\n",
    "### Momentum\n",
    "\n",
    "- Bonus: Implement momentum\n",
    "\n",
    "\n",
    "### Back to Keras\n",
    "\n",
    "- Implement the same network architecture with Keras;\n",
    "\n",
    "- Check that the Keras model can approximately reproduce the behavior of the Numpy model when using similar hyperparameter values (size of the model, type of activations, learning rate value and use of momentum);\n",
    "\n",
    "- Compute the negative log likelihood of a sample 42 in the test set (can use `model.predict_proba`);\n",
    "\n",
    "- Compute the average negative log-likelihood on the full test set.\n",
    "\n",
    "- Compute the average negative log-likelihood  on the full training set and check that you can get the value of the loss reported by Keras.\n",
    "\n",
    "- Is the model overfitting or underfitting? (ensure that the model has fully converged by increasing the number of epochs to 50 or more if necessary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/keras_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/keras_model_test_loss.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework assignments\n",
    "\n",
    "- Watch the following video on [how to code a minimal deep learning framework](https://www.youtube.com/watch?v=o64FV-ez6Gw) that feels like a simplified version\n",
    "of Keras but using numpy instead of tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"o64FV-ez6Gw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Optional**: read the following blog post on Reverse-Mode Automatic Differentiation from start to section \"A simple implementation in Python\" included:\n",
    "\n",
    "  https://rufflewind.com/2016-12-30/reverse-mode-automatic-differentiation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
